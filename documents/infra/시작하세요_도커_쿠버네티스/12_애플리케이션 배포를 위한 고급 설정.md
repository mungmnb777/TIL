# 애플리케이션 배포를 위한 고급 설정

날짜: 2023년 3월 7일
책: 시작하세요! 도커/쿠버네티스
카테고리: infra

## limits

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-pod
  labels:
    name: resource-limit-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    resources:
      limits:
        memory: "256Mi"
        cpu: "1000m"
```

- spec.containers.resources.limits : 메모리와 CPU 리소스를 제한할 수 있음 (상한선)

## requests

- 오버커밋 : 사용할 수 있는 자원보다 더 많은 양을 할당 → 전체 자원의 사용률 증가
- spec.containers.resources.limits : 컨테이너가 최소한으로 보장받아야하는 자원의 양

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-with-request-pod
  labels:
    name: resource-limit-with-request-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    resources:
      limits:
        memory: "256Mi"
        cpu: "1000m"
      requests:
        memory: "128Mi"
        cpu: "500m"
```

- 이 내용을 조합해보면 메모리는 최소 128메가, 그리고 여유가 있다면 256메가까지는 사용할 수 있다.
- CPU는 최소 500밀리, 여유가 있다면 1000밀리코어(1 cpu)까지 사용할 수 있다.
- 노드의 총 자원의 크기보다 더 많은 request를 할당할 수 없다. 쿠버네티스의 스케줄러는 pod의 request만큼 여유가 있는 노드를 선택한다.

## 메모리 자원 사용량 제한 원리

- CPU의 경합이 발생하면 스로틀을 이용해 사용을 제한할 수 있다.
- 메모리는 이미 데이터가 적재되어있기 때문에 압축할 수 없다.
    
    → 가용 메모리를 확보하기 위해 우선 순위가 낮은 pod 또는 프로세스를 강제 종료한다. 이후 다른 노드로 옮겨가게 되는데 이를 `퇴거(Eviction)`라고 한다.
    
- oom_score : 낮을수록 강제 종료 될 가능성이 작다. → 핵심 프로세스는 점수를 낮게 부여된다는데, 쿠버네티스가 어떤게 핵심 프로세스인지 어떻게 알 수 있을까?

## QoS

- Guaranteed 클래스 : Requests와 Limits가 완전히 동일할 때 적용되는 클래스
    - 기본 OOM 점수가 -998 → 웬만하면 강제로 종료되지 않음
- BestEffort 클래스 : resources 항목을 사용하지 않으면 적용되는 클래스
    - 노드에 존재하는 모든 자원을 사용할 수도 있지만, 자원을 전혀 사용하지 못할 수도 있다.
- Burstable 클래스 : Limits가 Requests보다 큰 파드에 적용되는 클래스
- 우선순위
    - Guaranteed → Burstable → BestEffort
    - 중요한 애플리케이션은 Guaranteed로 설정하는게 좋을듯

## ResourceQuota

- 네임스페이스에서 사용할 수 있는 자원 사용량을 제한

## LimitRange

- 특정 네임스페이스에 할당되는 자원의 범위 또는 기본값 설정

## 쿠버네티스 스케줄링

- kubelet, watch? 실시간으로 API 서버를 감시하는 느낌
- 노드 필터링, 노드 스코어링 - 노드를 선택하는 기법
- nodeSelector - 라벨의 키-값을 이용해 스케줄링
- Node Affinity
    - requiredDuringSchedulingIgnoredDuringExecution : 반드시 만족해야함
    - prefferedDuringSchedulingIgnoredDuringExecution : 해당 조건을 만족하는 노드를 더 선호함
- Pod Affinity : Topology Key에 적힌 라벨에 해당하는 그룹에 파드 생성
- PodAnti Affinity : Pod Affinity와 반대

### Taints & Tolerations

- Taints(얼룩) : 특정 노드에 얼룩을 지정 → 해당 노드에 파드가 할당되는 것을 막음
    - Label과의 차이로 `effect`를 가진다.
        - `NoSchedule` : 파드를 스케줄링하지 않음
        - `NoExecute` : 파드의 실행 자체를 허용하지 않음
        - `PreferNoSchedule` : 가능하면 스케줄링하지 않음
- Tolerations(용인) : 파드에 설정하면 Taints가 설정된 노드에도 파드를 할당할 수 있음
    
    → 노드에는 Taints, 파드에는 Tolerations
    
- Taints는 Key-Value를 가짐
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx-toleration-test
    spec:
      tolerations:
      - key: my-taint
        value: dirty              
        operator: Equal
        effect: NoSchedule
      containers:
      - name: nginx
        image: nginx:latest
    ```
    
    → alicek106/my-taint 키의 값이 `dirty`
    
    → Taint 효과가 NoSchedule인 경우 해당 `Taint`를 용인
    
- 그런데 실습이 잘 되지 않음..
    - kind-worker → `kubectl taint node kind-worker my-taint=dirty:NoSchedule`
    - kind-worker2 → `kubectl taint node kind-worker2 my-taint=clean:NoSchedule`
    - kind-worker3 → `kubectl taint node kind-worker3 my-taint=clean:NoSchedule`
    - 이렇게 설정 후 위의 yaml 파일을 실행해봄.
        
        ```
        NAME                    READY   STATUS    ...   NODE    ...  
        nginx-toleration-test   0/1     Pending   ...   <none>  ...
        ```
        
    - 근데 위와 같이 아무 노드에도 스케줄링 되지 않음 ㅠ ㅠ

- 마스터 노드에 파드가 할당되지 않는 이유도 `Taint` 때문임
    
    ```
    Name:               kind-control-plane
    
    ....
    
    Taints:             node-role.kubernetes.io/control-plane:NoSchedule
    Unschedulable:      false
    ```
    
- 노드에 장애가 발생했을 때도 Taint가 설정된다.
    - `node.kubernetes.io/not-ready:NoExecute` → NotReady Status일 때 파드가 실행되지 않음
    - `node.kubernetes.io/unreachable:NoExecute` → UnReachable Status일 때 파드가 실행되지 않음

### Cordon

- cordon에 의해 지정된 노드는 새로운 파드가 할당되지 않음
- NoSchedule인 Taint를 노드에 설정함 → `node.kubernetes.io/unschedulable:NoSchedule`
- 왜 쓸까?
    - 유지보수
    - 리소스 제한
    - 문제 복구 : 장애의 영향을 줄이기 위해서 사용

### Drain

- Drain에 의해 지정된 노드는 스케줄링되지 않고, 기존에 있던 파드들은 Eviction된다.
- 왜 쓸까?
    - 마찬가지로 유지보수 : 운영체제 업그레이드 등 아예 노드를 잠깐 종료해야할 때
    - 노드를 더 이상 사용하지 않는 경우 : 실행중인 파드를 안전하게 옮긴 후 노드를 제거할 수 있음

### PodDisruptionBudget

- drain을 사용할 때 특정 개수의 파드는 반드시 유지하기 위해 사용

---

## 쿠버네티스 애플리케이션 상태와 배포

### Deployment 롤링 업데이트

- Pod를 조금씩 삭제하고 업데이트 하는 방식
- maxSurce : 롤링 업데이트 중 전체 파드 개수가 Deployment의 replicas 값보다 얼마나 더 많아질 수 있는지
- maxUnavailable : 롤링 업데이트 중 사용 불가능한 파드의 최대 개수
- 이 친구를 이용해서 Blue-Green 배포도 가능하다.

### Pod의 Lifecycle

- Pending :
    - 파드가 생성되어 있지만, 실행할 노드가 아직 지정되지 않은 상태.
    - 스케줄링이 아직 완료되지 않았거나, 해당 노드가 사용 불가능한 상태일 때 발생 가능
- Running
    - 파드가 성공적으로 생성되어 노드에서 실행 중인 상태.
    - 이 상태에서 파드가 계속 실행되는지 모니터링할 수 있다. 만약 파드가 중지되면, 쿠버네티스가 자동으로 파드를 다시 시작한다.
    - 가장 바람직한 상태
- Complete
    - 파드가 성공적으로 실행되었지만, 작업이 완료되어서 종료된 상태.
    - 예를 들어, 한 번 실행되고 종료되는 일회성 작업을 수행하는 파드가 있는 경우 Completed 상태가 된다.
- Error
    - 파드 실행 중 문제가 발생하면, Error 상태가 된다.
    - 문제를 파악하고 해결해야 한다.
    - 예를 들어, 컨테이너가 시작되지 않거나, 다른 오류가 발생한 경우 Error 상태가 된다.
- Terminated
    - 파드가 정상적으로 종료되었거나, 문제로 인해 종료된 상태

### restartPolicy

- Pod가 Complete, Error일 때 적용
- Always : 컨테이너가 종료됐을 때 항상 다시 시작
- Never : 절대로 다시 시작하지 않음
- OnFailure : 컨테이너가 실패했을 때(0이 아닌 exit code)

### initContainers

애플리케이션이 실행되기 전에 먼저 실행되는 컨테이너

- 설정 파일을 가져오는 데 사용함

### Hook

- postStart : 컨테이너가 시작될 때 적용
    - HTTP : 특정 주소로 HTTP 요청 전소오
    - Exec : 컨테이너가 시작된 직후 특정 명령어 실행
        
        → Dockerfile의 ENTRYPOINT 역할을 하는듯
        
- preStop : 컨테이너가 종료될 때 적용

### livenessProbe

- 컨테이너 내부의 애플리케이션이 살아있는지 검사. 실패하면 restartPolicy의 정책을 따른다

### readinessProbe

- 컨테이너 내부의 애플리케이션이 사용자 요청을 처리할 준비가 됐는지 검사. 실패하면 서비스의 라우팅 대상에서 제외된다.
- httpGet : HTTP 요청으로 헬스 체크 → 200번대나 300번대 응답 코드가 와야함
- exec : 컨테이너 내부에서 명령어 실행 → 종료 코드가 0이 아니면 상태 검사 실패
- tcpSocket : TCP 연결이 수립될 수 있는지 체크

### HPA(Horizontal Pod Autoscaler)

- 디플로이먼트의 포드 개수를 자동으로 조절
- HPA는 실제 하드웨어가 Scale-Out 하는 것이 아니라서 실제 물리적인 공간이나 성능이 증가하는 것이 아닌데 왜 사용할까?